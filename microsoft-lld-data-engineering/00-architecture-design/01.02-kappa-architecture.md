# Kappa Architecture - Comprehensive Interview Guide

> **The "Stream Everything" Evolution**
> Simplification of Lambda Architecture by removing the Batch Layer entirely.

---

## ðŸ“– Table of Contents

1.  [Concept & Evolution](#concept--evolution)
2.  [Core Principles](#core-principles)
3.  [Architecture Deep Dive](#architecture-deep-dive)
4.  [Kappa vs Lambda](#kappa-vs-lambda)
5.  [The "Backfill" Challenge](#the-backfill-challenge)
6.  [Technology Choices](#technology-choices)
7.  [Interview Questions](#interview-questions)

---

## Concept & Evolution

### The Problem with Lambda
Lambda Architecture is robust but **complicated**. You maintain two codebases:
1.  **Batch Code** (e.g., Spark SQL running nightly)
2.  **Streaming Code** (e.g., Spark Streaming or Flink running continuously)
3.  **Serving Merge Logic** (Complex SQL to union them)

**Kappa Architecture** asks: *"If our stream processing engine is fast and accurate, why do we need a separate batch layer at all?"*

### The Kappa Solution
Treat **everything** as a stream.
*   Real-time data? It's a stream.
*   Historical data? It's just a stream that started a long time ago.
*   **Result**: One single code path for everything.

---

## Core Principles

### 1. Everything is a Stream (Unbounded Log)
Instead of thinking of "Daily Batches" files, Kappa thinks of an **Event Log** (Kafka) that retains data for a long time (or forever).

### 2. Single Codebase
You write your business logic **ONCE** using a stream processing framework (like Flink or Spark Structured Streaming). You run this code against real-time data or replay it against historical data.

### 3. Replay = Reprocessing
To "recompute" data (fixing a bug), you don't run a batch job. You simply:
1.  Spin up a new instance of your Streaming Job.
2.  Point it to the **beginning** of the input topic (Offset 0).
3.  Let it "catch up" high speed until it reaches the present.
4.  Switch the application to read from the new output.

---

## Architecture Deep Dive

```mermaid
graph LR
    Sources[Sources\nIoT, Web, DB] --> Kafka[(Unified Log\nKafka / Event Hub)]
    
    Kafka --> StreamJob[Stream Processing Engine\n(Flink / Spark Streaming)]
    
    StreamJob --> Serving[(Serving Layer\nSpeed Views)]
    
    Serving --> Apps[User Apps]
    
    subgraph "Reprocessing (The 'Replay')"
        StreamJobNew[New Version Code]
        Kafka -.-> |"Replay from Offset 0"| StreamJobNew
        StreamJobNew -.-> |"Write to New Table"| Serving
    end
```

### The Workflow
1.  **Ingest**: All data goes to Kafka (Log). Retention must be high (or tiered to S3).
2.  **Process**: A single stream job reads Kafka and updates the Serving Layer (e.g., updating a Redis counter or a Delta Table).
3.  **Serve**: The Serving Layer is simple. It just holds the calculated view. No complex "Merge" logic needed.

---

## Kappa vs Lambda

| Feature | Lambda | Kappa |
| :--- | :--- | :--- |
| **Complexity** | High (2 Codebases, Merge Logic) | Low (1 Codebase) |
| **Historical Data** | Batch layer (S3/Parquet) is source of truth | Kafka (Log) is source of truth* |
| **Reprocessing** | Run Batch Job (Easy, Standard) | Replay Stream (Harder at scale) |
| **Latency** | Mixed (Real-time + Batch) | Pure Real-time |
| **Cost** | Batch is cheap (transient clusters) | Streaming is 24/7 (always on) |

*\*Note: Modern Kappa often uses "Tiered Storage" where Kafka offloads old data to S3, so you don't actually keep petabytes on local disks.*

---

## The "Backfill" Challenge (The Kappa Weakness)

The #1 interview question against Kappa is: **"How do you reprocess 5 years of data?"**

### The Problem
Replaying 5 years of data through a Stream Processor (row-by-row) is **SLOW**. Batch jobs (vectorized, columnar) are much faster at crunching history.

### The Solution: "Hybrid" Replay
1.  **Code**: Write code using a unified API (e.g., Apache Beam or Spark Structured Streaming).
2.  **Execution**:
    *   For **History**: Run the code in "Batch Mode" against cold storage (S3).
    *   For **Now**: Run the *same* code in "Streaming Mode" against Kafka.
3.  **Result**: You get the simplicity of one codebase (Kappa) with the performance of Batch (Lambda). This is often called **"Lambda-less"** or **Delta Architecture**.

---

## Technology Choices

### 1. The Engine (Must handle Stream & Batch)
*   **Apache Flink**: The purest Kappa engine. True streaming, powerful state management.
*   **Spark Structured Streaming**: "Everything is a table". Great for the Hybrid approach.
*   **Kafka Streams**: Lightweight, code-only, runs inside your apps.

### 2. The Storage (Must support Replay)
*   **Apache Kafka**: With Tiered Storage enabled.
*   **Azure Event Hubs**: With "Capture" (Archive) enabled.
*   **Delta Lake**: Often used as the "Log" in modern architectures (Delta Architecture), replacing Kafka for the long-term history.

---

## Interview Questions

### Q: "When would you CHOOSE Lambda over Kappa?"
**A**: When the algorithms are fundamentally different.
*   Example: **Machine Learning Training**.
*   You cannot "stream" the training of a Deep Learning model efficiently. You need a massive Batch of static data to train (Lambda Batch Layer), while you perform Inference in real-time (Lambda Speed Layer). Kappa doesn't fit well here.

### Q: "How does Kappa handle late data?"
**A**: Since it's a stream engine, it handles late data natively using **Watermarks**.
*   The system keeps "State" (e.g., aggregation windows) open for a duration (e.g., 1 hour).
*   If data arrives 30 mins late, it updates the window logic correctly.
*   In Lambda, the Batch layer would just pick it up the next day. In Kappa, the Stream engine must be smart enough to handle it.

### Q: "Is Kappa cheaper?"
**A**: Usually **No**.
*   Streaming clusters run 24/7 (CPU/RAM reserved).
*   Batch clusters run transiently (Spin up, crunch, shut down).
*   Kappa optimizes for *Developer Time* (maintenance), not *Compute Cost*.
