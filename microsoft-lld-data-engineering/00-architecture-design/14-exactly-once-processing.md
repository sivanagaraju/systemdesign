# Exactly-Once Processing

> **Guaranteeing no duplicates and no data loss in streaming**

## The Core Problem

*"How do you guarantee each record is processed exactly once - not zero times, not twice?"*

```
Delivery Semantics:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
AT-MOST-ONCE:   May lose records (fire and forget)
AT-LEAST-ONCE:  May duplicate records (retry on failure)
EXACTLY-ONCE:   Each record processed exactly once ‚Üê THIS!
```

---

## üèóÔ∏è Exactly-Once Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    EXACTLY-ONCE PROCESSING ARCHITECTURE                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                              ‚îÇ
‚îÇ  THE CHALLENGE                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ                                                                          ‚îÇ‚îÇ
‚îÇ  ‚îÇ  Kafka/Event Hub ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Spark ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Delta Lake       ‚îÇ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ                            ‚îÇ                        ‚îÇ            ‚îÇ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ                            ‚îÇ FAILURE HERE!          ‚îÇ            ‚îÇ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ                            ‚ñº                        ‚îÇ            ‚îÇ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ            ‚îÇ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ                      ‚îÇ  Job    ‚îÇ                    ‚îÇ            ‚îÇ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ                      ‚îÇ Crashes ‚îÇ                    ‚îÇ            ‚îÇ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ            ‚îÇ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ                                                     ‚îÇ            ‚îÇ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ Did we commit offset?      Did we write to Delta?   ‚îÇ            ‚îÇ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ                                                     ‚îÇ            ‚îÇ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ If offset committed but no write ‚Üí DATA LOSS        ‚îÇ            ‚îÇ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ If write done but no commit ‚Üí DUPLICATES on retry   ‚îÇ            ‚îÇ‚îÇ
‚îÇ  ‚îÇ                                                                          ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                                                                              ‚îÇ
‚îÇ  SOLUTION: CHECKPOINTING + IDEMPOTENT WRITES                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ                                                                          ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   Kafka     ‚îÇ     ‚îÇ   Spark     ‚îÇ     ‚îÇ         Delta Lake          ‚îÇ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   Offsets   ‚îÇ     ‚îÇ Checkpoint  ‚îÇ     ‚îÇ     (ACID writes)           ‚îÇ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ                   ‚îÇ                           ‚îÇ               ‚îÇ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ                   ‚îÇ Stores:                   ‚îÇ               ‚îÇ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ                   ‚îÇ - Last processed offset   ‚îÇ               ‚îÇ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ                   ‚îÇ - Pending batch state     ‚îÇ               ‚îÇ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ                   ‚îÇ - Aggregation state       ‚îÇ               ‚îÇ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ                   ‚îÇ                           ‚îÇ               ‚îÇ‚îÇ
‚îÇ  ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ‚îÇ
‚îÇ  ‚îÇ                             ‚îÇ                                            ‚îÇ‚îÇ
‚îÇ  ‚îÇ                             ‚ñº                                            ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ WRITE-AHEAD LOG (WAL) Pattern:                                     ‚îÇ  ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                                                                    ‚îÇ  ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ 1. Read batch from Kafka (offsets 100-199)                        ‚îÇ  ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ 2. Write checkpoint: "Processing offsets 100-199"                  ‚îÇ  ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ 3. Transform data                                                  ‚îÇ  ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ 4. Write to Delta (atomic transaction)                             ‚îÇ  ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ 5. Update checkpoint: "Committed offsets 100-199"                  ‚îÇ  ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                                                                    ‚îÇ  ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ On restart: Check checkpoint, resume from last uncommitted batch   ‚îÇ  ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ‚îÇ
‚îÇ  ‚îÇ                                                                          ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                                                                              ‚îÇ
‚îÇ  WHY DELTA LAKE HELPS                                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ                                                                          ‚îÇ‚îÇ
‚îÇ  ‚îÇ  Delta Lake Properties:                                                  ‚îÇ‚îÇ
‚îÇ  ‚îÇ                                                                          ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚úÖ ATOMIC WRITES: Entire batch succeeds or fails together              ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚úÖ IDEMPOTENT: Re-writing same data has no effect (with proper config) ‚îÇ‚îÇ
‚îÇ  ‚îÇ  ‚úÖ EXACTLY-ONCE SINK: Spark streaming writes are transactional         ‚îÇ‚îÇ
‚îÇ  ‚îÇ                                                                          ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîß Code Implementation

### Spark Structured Streaming with Checkpoints

```python
from pyspark.sql.functions import *

# Read from Kafka with exactly-once
kafka_stream = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "broker:9092") \
    .option("subscribe", "orders") \
    .option("startingOffsets", "earliest") \
    .load()

# Parse and transform
orders = kafka_stream \
    .select(from_json(col("value").cast("string"), schema).alias("data")) \
    .select("data.*")

# Write to Delta with EXACTLY-ONCE guarantee
query = orders.writeStream \
    .format("delta") \
    .outputMode("append") \
    .option("checkpointLocation", "/checkpoints/orders_stream") \
    .option("mergeSchema", "true") \
    .trigger(processingTime="10 seconds") \
    .start("/silver/orders")

# The checkpoint directory contains:
# - offsets (which Kafka offsets were read)
# - commits (which batches were successfully written)
# - state (for stateful operations like aggregations)
```

### Idempotent Batch Processing

```python
from delta.tables import DeltaTable

def process_batch_idempotent(batch_id: str, input_path: str, output_path: str):
    """
    Process a batch idempotently - safe to re-run.
    """
    
    # Read input
    df = spark.read.format("delta").load(input_path) \
        .filter(f"batch_id = '{batch_id}'")
    
    # Transform
    result = df.withColumn("processed_at", current_timestamp())
    
    # Check if already processed (idempotency check)
    if DeltaTable.isDeltaTable(spark, output_path):
        existing = spark.read.format("delta").load(output_path) \
            .filter(f"batch_id = '{batch_id}'")
        if existing.count() > 0:
            print(f"Batch {batch_id} already processed, skipping")
            return
    
    # Write with MERGE (upsert) for idempotency
    if DeltaTable.isDeltaTable(spark, output_path):
        target = DeltaTable.forPath(spark, output_path)
        target.alias("target").merge(
            result.alias("source"),
            "target.record_id = source.record_id AND target.batch_id = source.batch_id"
        ).whenMatchedUpdateAll() \
         .whenNotMatchedInsertAll() \
         .execute()
    else:
        result.write.format("delta").save(output_path)
```

### Foreachbatch with Exactly-Once

```python
def write_to_multiple_sinks(batch_df, batch_id):
    """
    Write to multiple sinks with exactly-once semantics.
    """
    
    # Use batch_id for idempotency
    batch_df = batch_df.withColumn("batch_id", lit(batch_id))
    
    # Persist to avoid recomputation
    batch_df.persist()
    
    try:
        # Write to Delta (primary)
        batch_df.write.format("delta") \
            .mode("append") \
            .save("/silver/orders")
        
        # Write to metrics table
        metrics = batch_df.agg(
            count("*").alias("record_count"),
            sum("amount").alias("total_amount")
        ).withColumn("batch_id", lit(batch_id))
        
        metrics.write.format("delta") \
            .mode("append") \
            .save("/monitoring/batch_metrics")
        
    finally:
        batch_df.unpersist()


# Use foreachBatch for custom sink logic
query = orders.writeStream \
    .foreachBatch(write_to_multiple_sinks) \
    .option("checkpointLocation", "/checkpoints/orders_multi_sink") \
    .start()
```

---

## üìä Delivery Semantics Summary

| Semantic | Guarantee | How |
|----------|-----------|-----|
| **At-most-once** | Fire and forget | Commit offset before processing |
| **At-least-once** | May duplicate | Commit offset after processing |
| **Exactly-once** | Perfect | Checkpoint + atomic sink + idempotent writes |

---

## ‚ö†Ô∏è Requirements for Exactly-Once

| Requirement | Why | How |
|-------------|-----|-----|
| **Checkpoint enabled** | Track progress | `checkpointLocation` option |
| **Idempotent sink** | Safe re-writes | Delta Lake ACID, MERGE |
| **Deterministic processing** | Same input = same output | Avoid random(), current_timestamp() in transforms |

---

## üéØ Interview Questions

| Question | Expected Answer |
|----------|----------------|
| *"What is exactly-once?"* | Each record processed once - no loss, no duplicates |
| *"How does Spark achieve it?"* | Checkpoints (offsets + commits) + atomic Delta writes |
| *"What if job fails mid-batch?"* | Restart reads uncommitted offsets, replays processing |
| *"Why is Delta Lake important?"* | ACID writes ensure atomic commits, idempotent re-writes |

---

## üìñ Summary

You've now covered all **8 critical scenarios**:

1. ‚úÖ Out-of-Order Events (watermarks)
2. ‚úÖ Duplicate Records (deduplication)
3. ‚úÖ Schema Drift (evolution handling)
4. ‚úÖ Backfill (partition-by-partition)
5. ‚úÖ CDC Deletes (soft/hard delete)
6. ‚úÖ Data Reconciliation (validation)
7. ‚úÖ Hot Partitions (salting, AQE)
8. ‚úÖ Exactly-Once Processing (checkpoints)

**Good luck with your interview! üöÄ**
