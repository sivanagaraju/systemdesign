# Petabyte-Scale Patterns

> **When they say "we have petabytes of data"**

## The Core Question

*"We process petabytes of IoT data daily. How does your architecture handle this scale?"*

---

## ðŸ“Š Scale Considerations

| Scale | Daily Volume | Architecture Complexity |
|-------|--------------|------------------------|
| **Gigabytes** | < 100 GB | Simple, single notebook |
| **Terabytes** | 100 GB - 10 TB | Partitioning, parallel jobs |
| **Petabytes** | 10+ TB | Distributed, incremental, careful design |

---

## ðŸ—ï¸ PB-Scale Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PETABYTE-SCALE DATA ARCHITECTURE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  INGESTION & PARTITIONING                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                    Event Hub (100 partitions)                            â”‚â”‚
â”‚  â”‚                    â†“                                                     â”‚â”‚
â”‚  â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚â”‚
â”‚  â”‚         â”‚         Auto Loader (Streaming)             â”‚                  â”‚â”‚
â”‚  â”‚         â”‚         - maxFilesPerTrigger: 1000          â”‚                  â”‚â”‚
â”‚  â”‚         â”‚         - Parallel file discovery           â”‚                  â”‚â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â”‚  STORAGE PARTITIONING (Critical for PB scale!)                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  /bronze/iot_events/                                                     â”‚â”‚
â”‚  â”‚  â”œâ”€â”€ year=2024/                                                          â”‚â”‚
â”‚  â”‚  â”‚   â”œâ”€â”€ month=01/                                                       â”‚â”‚
â”‚  â”‚  â”‚   â”‚   â”œâ”€â”€ day=15/                                                     â”‚â”‚
â”‚  â”‚  â”‚   â”‚   â”‚   â”œâ”€â”€ hour=00/                                                â”‚â”‚
â”‚  â”‚  â”‚   â”‚   â”‚   â”‚   â””â”€â”€ *.parquet (target: 256MB each)                      â”‚â”‚
â”‚  â”‚  â”‚   â”‚   â”‚   â”œâ”€â”€ hour=01/                                                â”‚â”‚
â”‚  â”‚  â”‚   â”‚   â”‚   â””â”€â”€ ...                                                     â”‚â”‚
â”‚  â”‚  â”‚   â”‚   â””â”€â”€ day=16/                                                     â”‚â”‚
â”‚  â”‚  â”‚   â””â”€â”€ month=02/                                                       â”‚â”‚
â”‚  â”‚  â””â”€â”€ year=2025/                                                          â”‚â”‚
â”‚  â”‚                                                                          â”‚â”‚
â”‚  â”‚  Partition Key: year/month/day/hour (hierarchical)                       â”‚â”‚
â”‚  â”‚  File Size Target: 256MB - 1GB (avoid small files!)                      â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â”‚  PROCESSING STRATEGY                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  Never process ALL data at once!                                         â”‚â”‚
â”‚  â”‚                                                                          â”‚â”‚
â”‚  â”‚  âœ— WRONG: spark.read.format("delta").load("/bronze/iot_events")         â”‚â”‚
â”‚  â”‚  âœ“ RIGHT: .filter("event_date = '2024-01-15'")  â† Partition pruning     â”‚â”‚
â”‚  â”‚                                                                          â”‚â”‚
â”‚  â”‚  Incremental Processing:                                                 â”‚â”‚
â”‚  â”‚  - Track watermark (last processed timestamp)                            â”‚â”‚
â”‚  â”‚  - Process only new partitions since watermark                           â”‚â”‚
â”‚  â”‚  - Checkpoint progress for resume                                        â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â”‚  CLUSTER SIZING                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  For 1 PB / day processing:                                              â”‚â”‚
â”‚  â”‚  - Workers: 50-100 nodes                                                 â”‚â”‚
â”‚  â”‚  - Worker size: 16 cores, 64GB RAM each                                  â”‚â”‚
â”‚  â”‚  - Autoscaling: Min 20, Max 100                                          â”‚â”‚
â”‚  â”‚  - Spot/Preemptible: 70% to reduce cost                                  â”‚â”‚
â”‚  â”‚                                                                          â”‚â”‚
â”‚  â”‚  Calculation:                                                            â”‚â”‚
â”‚  â”‚  - 1 PB = 1,000 TB                                                       â”‚â”‚
â”‚  â”‚  - Processing rate: ~20 TB/hour per 50 nodes                             â”‚â”‚
â”‚  â”‚  - Time to process: ~50 hours (parallelize across partitions!)           â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ”„ Retry Policies at Scale

```python
# Databricks Workflow retry configuration
{
    "task_key": "bronze_to_silver",
    "retry_policy": {
        "max_retries": 3,
        "min_retry_interval_millis": 60000,    # 1 min
        "max_retry_interval_millis": 300000    # 5 min (exponential backoff)
    },
    "timeout_seconds": 7200,  # 2 hours max
    "email_notifications": {
        "on_failure": ["data-team@company.com"]
    }
}
```

---

## âš¡ Key Patterns for PB Scale

| Pattern | Why | How |
|---------|-----|-----|
| **Partition pruning** | Don't scan all data | Filter on partition columns first |
| **Incremental processing** | Process only new data | Track watermark, checkpoint |
| **File compaction** | Avoid small files problem | OPTIMIZE command, Auto-Optimize |
| **Z-Order** | Skip irrelevant rowgroups | Z-ORDER BY frequently filtered columns |
| **Adaptive Query Execution** | Handle skew automatically | `spark.sql.adaptive.enabled = true` |

---

## ðŸŽ¯ Interview Answer

> *"At petabyte scale, key considerations are:*
>
> 1. **Partition by time** (year/month/day/hour) for partition pruning
> 2. **Incremental processing** - never full table scans, use watermarks
> 3. **File size targets** - 256MB-1GB to avoid small files
> 4. **Autoscaling clusters** with spot instances for cost
> 5. **Retry with exponential backoff** for transient failures
> 6. **AQE enabled** for automatic skew handling"*

---

## ðŸ“– Next Topic

Continue to [Realistic Interview Questions](./06-realistic-interview-questions.md) for practice.
