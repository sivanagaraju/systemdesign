# Partitioning Algorithms

> Choosing the right algorithm for distributing data across nodes.

---

## ğŸ“‹ Overview

```mermaid
graph TB
    subgraph "Partitioning Algorithms"
        R[Range Partitioning<br/>Split by key ranges]
        H[Hash Partitioning<br/>Hash key mod N]
        CH[Consistent Hashing<br/>Hash ring]
    end
    
    R --> U1[Ordered data access]
    H --> U2[Even distribution]
    CH --> U3[Minimal rebalancing]
```

---

## 1ï¸âƒ£ Range Partitioning

> Assign continuous ranges of keys to each partition.

```mermaid
graph TB
    subgraph "Range Partitioned Database"
        M[Metadata Service<br/>A-F â†’ Node 1<br/>G-N â†’ Node 2<br/>O-Z â†’ Node 3]
        
        N1[Node 1<br/>Users A-F]
        N2[Node 2<br/>Users G-N]
        N3[Node 3<br/>Users O-Z]
    end
    
    Q[Query: Find 'Alice'] --> M
    M --> N1
```

### How It Works
1. Define key ranges for each partition
2. Maintain a mapping: range â†’ node
3. Route queries based on key lookup

### Advantages âœ…
- **Range queries are efficient**: Find all users 'A' to 'D' hits one node
- **Easy to understand**: Intuitive mapping
- **Dynamic splitting**: Split a range when it gets too big

### Disadvantages âŒ
- **Hot spots**: Sequential writes cluster on one partition
- **Uneven distribution**: Some ranges may have more data
- **Mapping overhead**: Must store and sync range metadata

### Real-World: HBase & Google BigTable

```mermaid
graph TB
    subgraph "HBase Region Servers"
        RS1[Region Server 1<br/>Keys: aaa-cho]
        RS2[Region Server 2<br/>Keys: cho-moz]
        RS3[Region Server 3<br/>Keys: moz-zzz]
    end
    
    HM[HMaster<br/>Tracks region assignments]
    
    HM --> RS1
    HM --> RS2
    HM --> RS3
```

**Auto-splitting**: When a region gets too large, HBase automatically splits it.

---

## 2ï¸âƒ£ Hash Partitioning

> Apply hash function to key, use modulo to find partition.

```mermaid
graph LR
    Key[user_id: 12345] --> Hash[hash: SHA256]
    Hash --> Mod[mod 4]
    Mod --> Partition[Partition 1]
```

**Formula**: `partition = hash(key) % num_partitions`

### Advantages âœ…
- **Even distribution**: Hash spreads data uniformly
- **No metadata needed**: Calculate partition at runtime
- **Prevents hot spots**: Sequential keys spread out

### Disadvantages âŒ
- **Range queries impossible**: Adjacent keys on different nodes
- **Rebalancing nightmare**: Changing N moves most data

### The Rebalancing Problem

```mermaid
graph TB
    subgraph "Before: 4 Nodes"
        B1[hash % 4 = 0 â†’ Node 0]
        B2[hash % 4 = 1 â†’ Node 1]
        B3[hash % 4 = 2 â†’ Node 2]
        B4[hash % 4 = 3 â†’ Node 3]
    end
    
    subgraph "After: 5 Nodes (disaster!)"
        A1[hash % 5 = 0 â†’ Node 0]
        A2[hash % 5 = 1 â†’ Node 1]
        A3[hash % 5 = 2 â†’ Node 2]
        A4[hash % 5 = 3 â†’ Node 3]
        A5[hash % 5 = 4 â†’ Node 4]
    end
    
    Note[~80% of keys<br/>change partition!]
    
    style Note fill:#ffcdd2
```

---

## 3ï¸âƒ£ Consistent Hashing

> Minimize data movement when nodes are added/removed.

### The Hash Ring

```mermaid
graph TB
    subgraph "Hash Ring [0-360]"
        R((Ring))
        
        N1["Node A @ 45Â°"]
        N2["Node B @ 135Â°"]
        N3["Node C @ 270Â°"]
        
        K1["Key 1 @ 30Â°<br/>â†’ Node A"]
        K2["Key 2 @ 100Â°<br/>â†’ Node B"]
        K3["Key 3 @ 200Â°<br/>â†’ Node C"]
    end
```

### How It Works

1. Hash nodes to positions on a ring (0 to 2^32 or 0 to 360)
2. Hash each key to the same ring
3. Key belongs to **first node clockwise** from its position

```mermaid
graph LR
    subgraph "Assignment Rule"
        K[Key Position: 100]
        N1[Node A: 45]
        N2[Node B: 135]
        N3[Node C: 270]
        
        K -->|Clockwise| N2
    end
```

### Adding a Node (Minimal Movement!)

```mermaid
graph TB
    subgraph "Before: 3 Nodes"
        BA[Node A: 45Â°]
        BB[Node B: 135Â°]
        BC[Node C: 270Â°]
    end
    
    subgraph "After: Add Node D at 180Â°"
        AA[Node A: 45Â°]
        AB[Node B: 135Â°]
        AD[Node D: 180Â° NEW]
        AC[Node C: 270Â°]
    end
    
    Note["Only keys 136Â°-180Â°<br/>move from C â†’ D!"]
    
    style Note fill:#c8e6c9
```

**Only ~1/N of keys move** on average (vs ~80% with hash mod N).

### Virtual Nodes (Vnodes)

Problem: Random node positions can cause uneven distribution.

Solution: Each physical node gets multiple positions on the ring.

```mermaid
graph TB
    subgraph "Virtual Nodes"
        PN1[Physical Node A]
        PN2[Physical Node B]
        
        V1[Vnode A1 @ 30Â°]
        V2[Vnode A2 @ 150Â°]
        V3[Vnode A3 @ 290Â°]
        
        V4[Vnode B1 @ 90Â°]
        V5[Vnode B2 @ 200Â°]
        V6[Vnode B3 @ 340Â°]
        
        PN1 --> V1
        PN1 --> V2
        PN1 --> V3
        
        PN2 --> V4
        PN2 --> V5
        PN2 --> V6
    end
```

**Benefits**:
- Better load distribution
- When node fails, its load spreads to multiple nodes
- Heterogeneous nodes can have proportional vnodes

---

## ğŸ”¥ Real-World: Amazon DynamoDB

```mermaid
graph TB
    subgraph "DynamoDB Consistent Hashing"
        Client[Client] --> Router[Request Router]
        
        Router --> VN1[Partition 1<br/>Vnodes: 10]
        Router --> VN2[Partition 2<br/>Vnodes: 10]
        Router --> VN3[Partition 3<br/>Vnodes: 10]
        
        VN1 --> R1[Replica 1]
        VN1 --> R2[Replica 2]
        VN1 --> R3[Replica 3]
    end
```

**Key Design Decisions**:
- Consistent hashing for partition assignment
- Virtual nodes for even distribution
- Replicas on consecutive nodes in ring
- Gossip protocol for membership

---

## ğŸ“Š Algorithm Comparison

| Feature | Range | Hash Mod N | Consistent Hashing |
|---------|-------|------------|-------------------|
| Range queries | âœ… Efficient | âŒ Scatter-gather | âŒ Scatter-gather |
| Data distribution | âš ï¸ Can be uneven | âœ… Even | âš ï¸ Even with vnodes |
| Adding/removing nodes | âš ï¸ Rebalance ranges | âŒ Most data moves | âœ… Minimal movement |
| Metadata overhead | âš ï¸ Range mapping | âœ… None | âš ï¸ Ring positions |
| Complexity | Low | Low | Medium |

---

## ğŸ¢ Which Systems Use What?

| System | Algorithm | Notes |
|--------|-----------|-------|
| HBase, BigTable | Range | For sorted access, auto-split |
| Redis Cluster | Hash slots (16384) | Fixed slots, manual assign |
| Cassandra | Consistent hashing + vnodes | Murmur3 hash |
| DynamoDB | Consistent hashing | With virtual nodes |
| MongoDB | Range or Hash | Configurable per collection |

---

## âœ… Key Takeaways

1. **Range partitioning** is best for ordered/range queries but risks hot spots
2. **Hash partitioning** distributes evenly but makes adding nodes painful
3. **Consistent hashing** minimizes data movement (~1/N on node changes)
4. **Virtual nodes** solve uneven distribution in consistent hashing
5. **Choose based on access patterns**: Range queries? Use range. Random access? Use hashing.

---

[â† Previous: Partitioning Strategies](./01-partitioning-strategies.md) | [Next: Replication Fundamentals â†’](./03-replication-fundamentals.md)
